# NLP Text Translation using LSTM

## Overview
This project implements an NLP-based sequence-to-sequence model using Long Short-Term Memory (LSTM) networks for language translation.  
It processes text data, tokenizes sequences, and trains an encoder-decoder architecture to translate sentences from one language to another.

## Features
- Text preprocessing and tokenization.  
- Encoderâ€“decoder architecture built with LSTM layers.  
- Sequence padding and attention-ready design.  
- Model training, validation, and evaluation.  
- Real-time translation testing for custom input.  

## Tech Stack
- Python  
- TensorFlow / Keras  
- NumPy, Pandas, Matplotlib  
- Natural Language Toolkit (NLTK)  

## Project Highlights
- Built a complete NLP pipeline from preprocessing to model evaluation.  
- Leveraged LSTM for capturing long-term text dependencies.  
- Designed for adaptability to multiple language pairs.  
